\chapter{Evaluation of PCE Selection Schemes}
\label{chap:EvaluationPCESelectionSchemes}
\markright{Evaluation of PCE Selection Schemes}
%======================================================================
\section{Background}
%======================================================================

\section{PCE Architecture}
The notion of Path Computation Element (PCE), originally named Path Computation Server (PCS), was initially introduced in order to solve the specific issue of inter-domain path computation for TE LSPs. The basic idea was to rely on the collaboration of ABRs or ASBRs to compute a TE LSP spanning multiple IGP areas or ASes.
The PCE framework [88] provides functions and protocol-extensions to address the computation of paths spanning multiple TE routing areas or administrative domains. There are several motivations for PCE-based architecture; among those are offloading the highly CPU-intensive path computation functions from control plane of core routers to some other devoted network routers or PCE(s).
Using the PCE architecture, prior to signaling an LSP that spans multiple domains, the TE LSP path is computed distributively and collaboratively– optionally applying various computational constraints– through communicating between cooperating PCEs usually residing at domain borders or exit points. Of particular interest in this chapter, is cooperative nature of PCE path computation. We show that depending on the information available to each PCE it is possible to enhance the total path computation time for the inter-domain LSP path, and consequently improve the blocking probability of the LSP.

The PCE is usually used as a generic term to define the overall architecture (specified in [88]), which is made of several components:
Traffic Engineering Label Switched Path (TE LSP): a unidirectional MPLS connection whose path is determined under certain QoS constraints by a TE module and defined as a collection of strict and/or loose hops.
The Path Computation Client (PCC): an entity originating a path computation request to a PCE (Path Computation Element) to compute a TE LSP. The PCC is typically an MPLS LSR (Label Switching Router).
The Path Computation Element (PCE): an entity (component, application or network node) that is capable of computing a network path or route based on a network graph and applying computational constraints.
PCE Protocol Signaling (PCEP): a PCC or PCE communicates with other PCE using a defined signaling protocol called PCEP and specified in [90].
PCE discovery: PCC(s) or PCE(s) can either use statically defined set of PCE(s) to consult for the path computation or discover the presence of PCEs and their capabilities using some discovery protocol (e.g. using extension to existing IGP flooding protocols like OSPF and ISIS).
Path Computation procedure: there are several PCE path computation techniques presented in the literature. Among those is the Backward Path Recursive Computation defined in [89]. As pointed out, the PCE can be used to cope with complex path computation problems in the context of intra-domain, inter-domain with partial domain visibility (case of inter-area or inter-Autonomous System (AS), or packet/non-packet domain in the context of GMPLS). In this document, we focus on the inter-domain MPLS TE case with BRPC.
PCE Policy: when PCE-based computation procedures are used to compute inter-AS TE LSP spanning multiple ASs managed by different Service Providers, policy becomes a key component of the architecture responsible for ensuring that path computation requests are directed to a specific PCE according to pre-established contract agreement (rate at which requests are sent, total amount of requested bandwidth, total number of TE LSPs, accounting, etc…). Furthermore, in some case, crossing AS boundaries require constraints mapping (bandwidth pool, preemption, affinity, etc…) should the two neighboring ASs make use of different conventions.
Traffic Engineering Database (TED): contains the topology and TE link resource information of the domain or area usually populated using extensions to IGP protocols like OSPF or ISIS, or other means (e.g. by a management application).

\subsection{PCE Path Computation Procedure}
One or more PCE routers typically residing at the domain or area borders are capable of performing optimal and/or diverse TE path computation for TE LSPs on behalf of other domain nodes usually referred to as PCCs. In the context of multi-layer networks, a PCE typically resides at the border of the adjacent layers (e.g. packet, optical, etc.) and accepts requests for hierarchical TE LSPs from the client upper layer (e.g. packet layer) and supplies the end-to-end multi-layer path.
When using PCE, the path computation for the TE LSP does not occur at the head-end (ingress) of the TE LSP. Instead, the head-end acting as a PCC determines and forwards the request with the desired path computation constraints to one of eligible PCEs capable of further processing the request (see Figure 1.2). The path computation request is then relayed until reaching the PCE that can resolve the full path to the inter-domain LSP’s destination node. 
Using BRPC, once the PCE determines that it can not solely compute the end-to-end path, it can consult other PCE(s) present neighboring domain(s) to do so.
If no next PCE can be found or if the next hop PCE of choice is unavailable, the procedure stops, and a path computation error is returned.  If multiple PCEs capable of serving the computation request are discovered, the upstream PCE may select a subset of these PCEs based on some local policies or heuristics (see Figure 1.3).

When a PCE receives a request for which it has direct visibility to the TE LSP destination node in its TED, it performs a local path computation for the shortest path from an ABR to the destination node, and replies back with the path to the upstream PCE. The upstream PCE in their turn append their own local domain paths to the full path and forward the PCE reply eventually to the PCC –the initiator of the request.
In the remaining of the chapter, we will study the effect of the PCE selection scheme on the path computation time and the LSP blocking probability. We will also present two heuristics to enhance this probability.


\subsection{PCE Selection for Path Computation}
Requests for path computation of an inter-area/domain TE LSP can be performed by either using a centralized PCE instance that has TE visibility over all of the other areas/domains, or can be distributed among multiple PCE instances –one responsible for each domain. In the latter case, when a PCE is not able to compute the full end-to-end path, a decision has to be made to select and forward the computation request to a downstream PCE node. This downstream PCE node selection process is crucial in the amount of overall time taken to compute the full end-to-end path. Typically, routing information – e.g. reachability to the destination announced by area border routers or autonomous system border routers or another user set routing policy— is used to define a set of eligible PCEs that are capable of processing further the path computation request.

However, among the set of candidate PCE(s), the decision to elect a certain PCE and forward the path computation request can affect significantly the overall end-to-end path computation response time depending on the degree of congestion on this PCE and the available resources for the computation process. Consequently, as the path computation time increases significantly, the signaled LSP can be potentially blocked at transit nodes due to reasons like: 1) the PCC can timeout waiting for the expected path reply after crossing a certain acceptable time threshold, 2) due to network state dynamics in each of the traversed domain(s) due to resources no longer available (e.g. due to other competing intra/inter-area LSPs), specifically after a considerable time had elapsed from the time of last path computation in that domain.

\section{Proposed PCE selection schemes}
There are a number of schemes that can be considered to elect a preferred PCE from a set of candidates that can collaboratively compute the overall end-to-end inter-area or domain TE LSP’s path. In this section we present two approaches that attempt to forward the PCE requests to PCEs that are less loaded.
In the first approach, the selection decision is done on per PCE-hop to determine the next hop PCE to further process the path computation request. Each PCE keeps a record for the performance measure to peering PCE(s) (e.g. the path computation response time) and utilizes this in its decision making. We refer to this approach as per-Hop PCE selection.
The second approach relies on keeping an average of the response time for each downstream PCE and sharing this state with a tier-1 PCE instance that overlooks over all other areas. We refer to this as source-specified PCE approach (SSPCE).
We describe in more details the 2 approaches and present some results for simulation runs that we collected on the topology described in Figure 1.9.

\subsection{Analysis and Mathmatical Model}
We assume that each PCE can be modeled as an $M/M/1$ queue where path computation requests arrive at rate $\lambda_p$. Each PCE takes, on average, time $T_p$ to process a request with the average path computation serviced rate $\mu=\frac{1}{T_p}$. The expected response time at $PCE_p$ can be written as:

\begin{equation}
E(t)=\dfrac{1}{\mu_p-\lambda_T^p}
\end{equation}

and  $\lambda_T^p$ being the total rate of arrival of path computation requests from all possible upstream PCE(s) to node $p$.

\begin{equation}
\lambda_T^p=\sum_{i=1}^n\lambda_i^p
\end{equation}

The expected number of requests R queued in PCE p can be written as:
\begin{equation}
E(R)=\dfrac{\lambda_T^p}{\mu_p-\lambda_T^p}
\end{equation}

Considering the case where arriving requests at a PCE node can be forwarded to $m$ downstream PCEs. The probability that any particular PCE request is directed to a particular PCE is $\frac{1}{m}$. It follows the probability that exactly $x$ out of $R$ requests are directed to that PCE is:

\begin{equation}
P(x)=
\begin{pmatrix}
R  \\ 
x  
\end{pmatrix}
m^{-R} (m-1)^{R-x}
\end{equation}

where,
\begin{equation}
\begin{pmatrix}
R  \\ 
x  
\end{pmatrix} = \dfrac{R!}{x!(R-x)!}
\end{equation}

The following properties then apply:
\begin{enumerate}
\item The stability condition is given by $\lambda_T \langle m\mu$.
\item The quantity $\rho=\frac{\lambda_T}{m\mu}$  gives the utilization of a single server.
\item The steady-state probabilities for $\rho \langle 1$ are given by:

\begin{equation}
p_0= \left[ \sum_{k=0}^{m-1} \dfrac{(m\rho)^k}{k!} +
\left( \dfrac{(m\rho)^m}{m!}\right) 
\left( \dfrac{1}{1-\rho}\right) 
 \right]^{-1}
\end{equation}

\begin{equation}
p_k= \begin{cases} 
p_0 \dfrac{(m\rho)^k}{k!}, & k\leq m \\
p_0 \dfrac{m^m\rho^k}{m!}, & k \geq m
\end{cases}
\end{equation}

\end{enumerate}

For example, consider the case where a PCE takes on average $T=2.5\,milliseconds$ to complete a TE path computation transaction, (i.e. requests are serviced at rate $\mu=400\,req/sec$), Figure XXX shows the expected number of PCE requests queued as a function of load utilization $\rho$. As the total rate arrival of PCE requests ?T approaches the rate of service the path computation requests $\mu$, the number of queued PCE requests and subsequently the expected response time increases significantly.

Within a time period $t_x$, assuming $x$ requests arrive at a certain PCE $p$, as long as $xT < t_x$ there will be no queuing of requests at PCE $p$; the latency experienced in this case for each request will be $T$, usually negligible. When $xT > t_x$, the request processing rate is limited by the server, each request averaging one transaction in each period $xT$. The extra delay per each request transaction is $(xT - t_x)$.

We can observe from previous discussion that the number of redundant/eligible PCEs m that can process a certain path computation request plays a key role in bounding the maximum response time experienced by the request. In general, the base (minimum adequate) number of PCEs must provide acceptable response time, so must not be very far into the overload region.

\subsubsection{Source Specified PCE Selection}
This scheme assumes that each PCE keeps track of mean aggregate arrival rate of path computation requests and uses this information to periodically flood a metric associated with the cost of using that PCE for a path computation. Each PCE creates a PCE virtual topology in the form of a graph $G(V,E)$ where $V$ corresponds to the set of PCEs in all areas, and $E$ the set of virtual links/edges between peering PCEs and whose link-weights reflect the amount of system utilization for that downstream PCE (see Figure 1.7). In summary, PCEs that are highly utilized will flood a large metric to be used for the virtual links and consequently will be avoided in any future path computations.

The originator of the PCE request (e.g. PCC or PCE) directs the PCE request to a PCE in its domain. In turn, the PCE computes the least PCE utilized path that is capable of computing the end-to-end inter-area path. The PCE-hops sequence can then be carried in the PCEP path computation request message and be used at each PCE hop to forward the requests downstream

In order to define a meaningful link cost associated with expected response time when using that PCE, we attempt to model each PCE as an $M/M/1$ queue. 
Equation 5.7 defines the PCE utilization $(\rho_p)$ which represents the amount of load at any PCE router $p$. It is defined as the ratio of the total request arrival rate $\lambda_T^p$ to the request service $rate \mu_p$:

\begin{equation}
\rho_p = \dfrac{\lambda_T^p}{\mu_p}
\end{equation}

The load intensity on each PCE which serves as an indication of the total number of requests queued awaiting processing at PCE $p$ is shown in Equation 1.9.  Note as $\rho\longrightarrow1$, the cost the virtual link to use PCE $p$ increases dramatically. This means that as the PCE server queue grows in size the PCE is penalized by increasing this cost so that incoming requests can use other PCEs to compute their path.

\begin{equation}
c_p=\dfrac{\rho_p}{1-\rho_p}
\end{equation}

\section{Per-hop PCE Selection}
This approach assumes the PCE selection is performed at each traversed PCE hop to elect a preferred next hop PCE to further process the request. Along this vein we present three methods to partition the requests: 1) by equal partitioning (e.g. round-robin distribution), 2) by forwarding requests to the PCE with least response time, and 3) using an adaptive approach to partition the requests using token quotas defined based on an average response time for each peering PCE.

\subsection{Round-robin PCE selection}
The Round-robin PCE seclection (RRS) method assumes that requests are distributed equally in a round-robin fashion among a number of eligible PCEs that are capable of processing further the path computation request. In this scheme, requests from a certain source can be assumed to be locally distributed evenly among the available candidate PCEs. However, this does not guarantee global request balancing among the all candidate PCEs, and hence, can lead to some PCEs being overloaded with large queue of requests leading to increased delays in the overall path computation response.

\subsection{Least response PCE selection}
The Least Response PCE (LRS) selection heuristic assumes that each PCE preserves locally an average path computation response time to each peering PCE. The downstream PCE would be always picked based on the least PCE response time. This scheme will achieve relatively better load request load balancing among PCEs. However, depending on the accuracy of the average response time stored locally, might not always yield the best PCE to forward to.

\subsection{Token-based PCE selection}
The Token-based PCE (TBS) selection heuristic also assumes an average response time is kept to each of the peering PCE(s). Arriving requests are partitioned among the eligible PCEs based on a token-based quota policy that is define based on the recorded average response times for each peering PCE. The main idea behind this heuristic is to penalize the PCE with the higher path computation response time by sending to it less path computation requests at any one time. Note, we assume in this case that the partitioning quotas (tokens) can be updated periodically or once every time a path computation reply is received from downstream PCE. Algorithm 1 shows the steps included in executing this algorithm.

\begin{algorithm}
\caption{Token-based PCE selection heuristic}  % give the algorithm a caption
\label{alg.TBS}
\begin{algorithmic}                    % enter the algorithmic environment
\WHILE{More PCE path compute request in queue}
\STATE Initialize token vector $T$ for all peering PCEs:
\STATE $T \Leftarrow \phi$
\STATE Find $S_r$ set of eligible PCEs for request $R$
\STATE Extract PCE $p$ with highest request tokens
\STATE Forward path compute request to $p$
\STATE Decrement PCE request tokens for PCE $p$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

